create database hive_class_b1;

use hive_class_b1;

create table department_data                                                                                                            
    > (                                                                                                                                       
    > dept_id int,                                                                                                                            
    > dept_name string,                                                                                                                       
    > manager_id int,                                                                                                                         
    > salary int)                                                                                                                             
    > row format delimited                                                                                                                    
    > fields terminated by ','; 

# Display column name
set hive.cli.print.header = true;
    
describe department_data;

describe formatted department_data;

# For data load from local
load data local inpath 'file:///tmp/hive_class/depart_data.csv' into table department_data;

# Load data from hdfs location
load data inpath '/tmp/hive_data_class_2/' into table department_data_from_hdfs;

Internal Table: Data Moves to Hive Warehouse
 - When we load data into an internal table, Hive moves the data from the HDFS location to the table's default location, which is inside the Hive warehouse             (/user/hive/warehouse/<table_name>).

 - The original data file is removed from its original location in HDFS after the load.

 - This means Hive takes full ownership of the data, and if the table is dropped, the data is also deleted.

External Table: Data Remains in Place
 - When loading data into an external table, Hive does not move the data; it stays in the original HDFS location specified during table creation.
 - The table only contains metadata pointing to that location.
 - Dropping an external table does not delete the actual data, only the metadata.

This behavior ensures that:

 - Internal tables are fully managed by Hive.
 - External tables allow data sharing across multiple tools without moving or duplicating files.

# Creating External table 
create external table de_data_ext                                                                                          
    > (                                                                                                                                       
    > dept_id int,                                                                                                                            
    > dept_name string,                                                                                                                       
    > manager_id int,                                                                                                                         
    > salary int                                                                                                                              
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > location '/tmp/hive_data/';

# in location we have provided dirctory path insted of file because whenever new files come in this directory it automatically reads that files

# work with Array data types
# create table emp                                                                                                                   
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > name string,                                                                                                                            
    > skills array<string>                                                                                                                    
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > collection items terminated by ':';

# collection items terminated by ':' this we ahve used becasue skills column chetan:ishwar:patil like this

# loading data from local to table
load data local inpath 'file:///tmp/hive_class/array_data.csv' into table emp; 

# to select first emelent from an array we can use
select id,name,skills[0] as prime_skill from emp

#calculating size and array containing HADOOP
select                                                                                                                                  
    > id,                                                                                                                                     
    > name,                                                                                                                                   
    > size(skills) as size_of_each_array,    # size is calcluate size of array                                                                                      
    > array_contains(skills,"HADOOP") as knows_hadoop,  # array_contains works like regex                                                                           
    > sort_array(skills) as sorted_array  # this function is for sorting array                   
    > from employee; 

# table for map data ( map means diectinary in python like key vlaue pair)
create table emp_map_data                                                                                                          
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > name string,                                                                                                                            
    > details map<string,string>                                                                                                              
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > collection items terminated by '|'                                                                                                      
    > map keys terminated by ':';
    
 load data local inpath 'file:///tmp/hive_class/map_data.csv' into table emp_map_data;


 # map functions
 select                                                                                                                                  
    > id,                                                                                                                                     
    > name,                                                                                                                                   
    > details,                                                                                                                                
    > size(details) as size_of_each_map,                                                                                                      
    > map_keys(details) as distinct_map_keys,                                                                                                 
    > map_values(details) as distinct_map_values                                                                                              
    > from employee_map_data; 

